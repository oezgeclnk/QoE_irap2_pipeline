#!/usr/bin/python3

#######################################################################
#---------------------------------------------------------------------#
#Import Libraries
#---------------------------------------------------------------------#

from __future__ import division
from random import *
from math import *

from multiprocessing import Process, Queue
import requests
from requests.auth import HTTPBasicAuth

import time
from datetime import datetime
import subprocess					#To invoke bash commands and background scripts
import sys, os
import copy
import struct

import dpkt
import socket
import netifaces as ni

import fnmatch 						#Count Pcap files in directory
import csv
from io import StringIO				#For default capinfos function

import numpy as np
import pandas as pd
import gc
import config						#User-defined config.py file

from sklearn.ensemble import RandomForestClassifier
from joblib import dump, load		#Load and save model data

#---------------------------------------------------------------------#
#######################################################################

#######################################################################
#---------------------------------------------------------------------#
#Global Variables
#---------------------------------------------------------------------#

NETWORK_INTERFACE = config.NETWORK_INTERFACE
T_OBSERVATION = config.T_OBSERVATION
TCPDUMP_FILENAME_PREFIX = config.TCPDUMP_FILENAME_PREFIX
FINAL_CSV_PREFIX = config.FINAL_CSV_PREFIX
MAX_FILE_COUNT = config.MAX_FILE_COUNT

#---------------------------------------------------------------------#
#######################################################################

#######################################################################
#---------------------------------------------------------------------#
#Global Functions
#---------------------------------------------------------------------#

#Define Queue for sharing data between processes
q_statsCollector = Queue()
q_finalData = Queue()
q_MLData = Queue()

print("Queues Defined")

#---------------------------------------------------------------------#
#######################################################################

#######################################################################
#---------------------------------------------------------------------#
#Threads and Processes
#---------------------------------------------------------------------#

#---------------------------------------------------------------------#
#File Count Monitor
#https://stackoverflow.com/questions/2632205/how-to-count-the-number-of-files-in-a-directory-using-python
class FileCountMonitor(Process):
	'''
	Python process which counts number of files in current directory and 
	deletes the earliest captured file after MAX_FILE_COUNT is reached
	
	**
	MAX_FILE_COUNT is defined in config.py
	'''
	def __init__(self, processID):
		Process.__init__(self)
		self.processID = processID
		
	def run(self):
		print("\n***** File Count Monitor Activated *****\n")
		dirpath=os.getcwd()
		print(dirpath)
		while True:
			try:
				total_file_count = len(fnmatch.filter(os.listdir(dirpath), '*.pcap'))
				if (total_file_count > MAX_FILE_COUNT):
					os.remove(min(fnmatch.filter(os.listdir(dirpath), '*.pcap'), key = os.path.getctime))
				time.sleep(T_OBSERVATION)
				
			except KeyboardInterrupt:
				break
#---------------------------------------------------------------------#

#---------------------------------------------------------------------#
#Statistic Collector Agent captures statistics from TCPDUMP
#https://stackoverflow.com/questions/415511/how-to-get-the-current-time-in-python/28576383
class StatisticCollector(Process):
	'''
	Periodically reads directory according to T_OBSERVATION
	and import pcap file generated by tcpdump
	
	**
	T_OBSERVATION is defined in config.py
	'''
	def __init__(self, processID):
		Process.__init__(self)
		self.processID = processID
	
	def run(self):
		while True:
			try:
				current_time = datetime.now().strftime('%H-%M-%S')
				file_name = TCPDUMP_FILENAME_PREFIX+'-'+current_time+'.pcap'
				time.sleep(T_OBSERVATION)
				if os.path.exists(file_name):
					q_statsCollector.put(file_name)					#Put collected data/raise a flag to queue of next thread
				
			except Exception as ex:
				template = "An exception of type {0} occured. Arguments:\n{1!r}"
				message = template.format(type(ex).__name__, ex.args)
				print(message)
				print('\n StatisticCollector | There is an error!!')
				break
#---------------------------------------------------------------------#

#---------------------------------------------------------------------#
#DataPreprocessor Thread
class DataPreprocessor(Process):
	'''
	Pre-processes data for Machine Learning algorithms by generating
	required features from imported Pcap files
	'''
	def __init__(self, processID):
		Process.__init__(self)
		self.processID = processID
	
	def feature_generation(self, file_name):
		
		#IPv4 and IPv6 address of interface
		ipv4 = ni.ifaddresses(NETWORK_INTERFACE)[ni.AF_INET][0]['addr']
		ipv6 = ni.ifaddresses(NETWORK_INTERFACE)[ni.AF_INET6][0]['addr'].split('%')[0]
		ips = [ipv4, ipv6]
		
		#Time Lists
		(lst_tot_ts, lst_fwd_ts, lst_bwd_ts, fwd_pkts_all, bwd_pkts_all) = ([], [], [], [], [])

		# Packet Counters
		(nonipcounter, tot_pkts, tot_bytes, fwd_pkts, fwd_bytes, bwd_pkts, bwd_bytes) = (0, 0, 0, 0, 0, 0, 0)
		
		#Timestamp of pcap
		starttime = None
		
		#Pcap Statistics
		pcapstats = {}
		pcap = dpkt.pcap.Reader(open(file_name,'rb'))

		# Packet processing loop
		for ts,pkt in pcap:
			
			if not starttime:
				starttime = str(datetime.fromtimestamp(ts))
			
			tot_pkts+=1
			lst_tot_ts.append(ts)
			
			#Parse packet
			try:
				if pcap.datalink()==dpkt.pcap.DLT_LINUX_SLL:	#Handle Linux Cooked Capture
					eth=dpkt.sll.SLL(pkt)
					pkt_type=eth.ethtype
				else:
					eth=dpkt.ethernet.Ethernet(pkt)
					pkt_type=eth.type
			except:
				continue
			 
			#check if IP packet or non-ip packet
			if pkt_type == dpkt.ethernet.ETH_TYPE_IP or pkt_type == dpkt.ethernet.ETH_TYPE_IP6:
				
				ip=eth.data
				tot_bytes+=len(eth)
				
				# Extract source IP of packet
				if pkt_type==dpkt.ethernet.ETH_TYPE_IP6:					# IPV6 packets
					try:
						src_address = socket.inet_ntop(socket.AF_INET6, ip.src)
					except:
						print(f'Error in packet - {tot_pkts}, size - {len(eth)}')		
				elif pkt_type==dpkt.ethernet.ETH_TYPE_IP:					# IPV4 packets
					try:
						src_address = socket.inet_ntoa(ip.src)
					except:
						print(f'Error in packet - {tot_pkts}, size - {len(eth)}')
					
				#Decide uplink/downlink
				if(src_address in ips):		#Uplink
					fwd_pkts+=1
					fwd_bytes+=len(eth)
					fwd_pkts_all.append(len(eth))
					lst_fwd_ts.append(ts)
					
				else:							#Downlink
					bwd_pkts+=1
					bwd_bytes+=len(eth)
					bwd_pkts_all.append(len(eth))
					lst_bwd_ts.append(ts)
				
			else:
				nonipcounter = nonipcounter + 1

		#Calculate additional features
		fwd_avg_bytes = (fwd_bytes / fwd_pkts) if fwd_pkts > 0 else 0
		bwd_avg_bytes = (bwd_bytes / bwd_pkts) if bwd_pkts > 0 else 0
		
		fwd_pkt_max = max(fwd_pkts_all) if len(fwd_pkts_all) > 0 else 0
		fwd_pkt_min = min(fwd_pkts_all) if len(fwd_pkts_all) > 0 else 0
		bwd_pkt_max = max(bwd_pkts_all) if len(bwd_pkts_all) > 0 else 0
		bwd_pkt_min = min(bwd_pkts_all) if len(bwd_pkts_all) > 0 else 0
		
		lst_tot_iat=[x - lst_tot_ts[i - 1] for i, x in enumerate(lst_tot_ts) if i > 0]
		tot_avg_iat = (sum(lst_tot_iat)/len(lst_tot_iat)) if len(lst_tot_iat) > 0 else 0

		lst_fwd_iat=[x - lst_fwd_ts[i - 1] for i, x in enumerate(lst_fwd_ts) if i > 0]
		fwd_avg_iat = sum(lst_fwd_iat)/len(lst_fwd_iat) if len(lst_fwd_iat) > 0 else 0

		lst_bwd_iat=[x - lst_bwd_ts[i - 1] for i, x in enumerate(lst_bwd_ts) if i > 0]
		bwd_avg_iat = sum(lst_bwd_iat)/len(lst_bwd_iat) if len(lst_bwd_iat) > 0 else 0

		#Append final stats
		pcapstats['Start time']=starttime
		pcapstats['Total packets']=tot_pkts
		pcapstats['Total bytes']=tot_bytes
		pcapstats['Total forward packets']=fwd_pkts
		pcapstats['Total forward bytes']=fwd_bytes
		pcapstats['Average forward bytes']=fwd_avg_bytes
		pcapstats['Total backward packets']=bwd_pkts
		pcapstats['Total backward bytes']=bwd_bytes
		pcapstats['Average backward bytes']=bwd_avg_bytes
		pcapstats['Max forward packet size']=fwd_pkt_max
		pcapstats['Min forward packet size']=fwd_pkt_min
		pcapstats['Max backward packet size']=bwd_pkt_max
		pcapstats['Min backward packet size']=bwd_pkt_min
		pcapstats['Average total IAT']=tot_avg_iat
		pcapstats['Average forward IAT']=fwd_avg_iat
		pcapstats['Average backward IAT']=bwd_avg_iat
		
		#Create Pandas Dataframe
		df = pd.DataFrame([pcapstats])
		return df
	
	def run(self):
		df_final = pd.DataFrame()
		df_stats = pd.DataFrame()
		df_stats_prev = pd.DataFrame()
		initialFlow = True
		while True:
			try:
				if not q_statsCollector.empty():		#Observe data/flag to process this thread
					
					file_name = q_statsCollector.get()	#Collect TCPDUMP data (pcap file)
					
					# Perform data aggregation here
					chosen_file = open(file_name, 'rb')
			
					#DPKT statistics calculator
					df_stats = self.feature_generation(file_name)
					if initialFlow == True:
						df_stats_prev = df_stats.iloc[:,1:].add_suffix('_prev')
						df_stats_prev = df_stats_prev.replace(df_stats_prev, 0)
						initialFlow = False
					df_final = pd.concat([df_stats, df_stats_prev], axis=1)
					q_finalData.put(df_final.iloc[0:])
					df_stats_prev = df_stats.iloc[:,1:].add_suffix('_prev')
					
			except Exception as ex:
				template = "An exception of type {0} occured. Arguments:\n{1!r}"
				message = template.format(type(ex).__name__, ex.args)
				print(message)
				print('\nDataPreprocessor shows an error!!')
				break		
#---------------------------------------------------------------------#

#---------------------------------------------------------------------#
#Macine Learning Thread
class MachineLearningAlgorithm(Process):
	'''
	Loads stored machine learning models and scalars and predicts
	Quality of Experience in real-time
	
	'''
	def __init__(self, processID):
		Process.__init__(self)
		self.processID = processID
	
	def load_model_data(self):
		scaler = load('/home/oce/Desktop/run_pipeline/scalers/scaler_df13_092020.joblib')		#Load stored scaler
		rf_clf = load('/home/oce/Desktop/run_pipeline/models/mlp_model_092020_bestparams.joblib')				#Load Model
		
		return (scaler, rf_clf)
	
	def run(self):
		count = 0
		(scaler, rf_clf) = self.load_model_data()
		print('**** Stored model data successfully loaded ****\n')
		while True:
			try:
				if not q_finalData.empty():				#Observe data/flag to process this thread
					count = count + 1
					rcd_flow = q_finalData.get()
					# ~ # Perform ML operations here
					req_flow = rcd_flow.iloc[:,1:]
					req_flow = np.reshape(req_flow, (1,-1))
					req_flow = scaler.transform(req_flow)
					prediction = rf_clf.predict(req_flow)                                                                                                                                     
					print('\nCurrent prediction - {}'.format(prediction))
					rcd_flow = rcd_flow.assign(Prediction = '')
					rcd_flow[['Prediction']] = prediction
					print('\n Current flow and corresponding prediction \n {} \n'.format(rcd_flow))
					q_MLData.put(rcd_flow)
					
			except Exception as ex:
				template = "An exception of type {0} occured. Arguments:\n{1!r}"
				message = template.format(type(ex).__name__, ex.args)
				print(message)
				print('\nMachineLearningAlgorithm shows an error!!')
				break	
#---------------------------------------------------------------------#
#######################################################################

#######################################################################
#---------------------------------------------------------------------#
#Main Thread
#---------------------------------------------------------------------#

def main():
	'''
	Main Thread which initialises processes and starts tcpdump process
	in background
	
	Tcpdump process captures traffic on NETWORK_INTERFACE for T_OBSERVATION
	seconds and stores generated Pcap files in the form -
	TCPDUMP_FILENAME_PREFIX-current_time
	
	At end of code execution after pressing Ctrl+C, all collected data 
	with predictions is stored to a CSV file in the form -
	FINAL_CSV_PREFIX-current_time
	
	**
	NETWORK_INTERFACE is defined in config.py
	T_OBSERVATION is defined in config.py
	TCPDUMP_FILENAME_PREFIX is defined in config.py
	FINAL_CSV_PREFIX is defined in config.py
	'''
	DFmain_stats = pd.DataFrame()
	
	#Start Processes
	p1 = FileCountMonitor(1)
	p1.daemon = True			#Experimental
	p1.start()
	
	p2 = DataPreprocessor(2)
	p2.daemon = True
	p2.start()
	
	p3 = MachineLearningAlgorithm(3)
	p3.daemon = True
	p3.start()
	
	p4 = StatisticCollector(4)
	p4.daemon = True
	p4.start()
	
	proc = [p1, p2, p3, p4]
	
	print("Process defined")
	
	subprocess.check_call("./tcpdump_capture_002.sh %s %s %s" %(NETWORK_INTERFACE, 
		str(T_OBSERVATION), TCPDUMP_FILENAME_PREFIX), shell=True) #Run in background
	
	try:
		while True:
			if not q_MLData.empty():
				df_stats = q_MLData.get()
				if DFmain_stats.empty:
					DFmain_stats = df_stats
				else:
					DFmain_stats = DFmain_stats.append(df_stats, ignore_index=True)
				
	except KeyboardInterrupt:
		for p in proc:
			p.terminate()
		#print(DFmain_stats)
		current_time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
		DFmain_stats.to_csv(FINAL_CSV_PREFIX+'-'+current_time+'.csv',index=False)
		gc.collect()
		input("\nPROGRAM FINISHED! PRESS ENTER TO FINISH!") 
		print("\nEXITING........!")
		
#---------------------------------------------------------------------#
#######################################################################

if __name__ == '__main__':
    main()

#######################################################################
